{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFewrn1Lk60Cy/w0maCw0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetAmarAtGithub/Research-Papers/blob/main/Text_Summary/Text_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The proposed work on \"opinion summarization\" using abstractive-based and extractive-based techniques. The abstractive-based approach involves constructing graphs from text and generating candidate summaries by exploring graph properties and considering sentiment fusion. The extractive-based approach aims to reduce dimensions and find summary sentences based on thematic words and ranking.\n",
        "\n",
        "The proposed algorithm consists of several steps. In the abstractive-based approach, it starts with constructing graphs where nodes represent tokens in the text and edges represent word adjacency. The accuracy of the constructed sentences is ensured by following a set of rules. Sentences are scored, sentiments are merged, and redundant sentences are removed. Finally, the sentences are ranked for summarization.\n",
        "\n",
        "In the extractive-based approach, the algorithm utilizes PCA (Principal Component Analysis) to reduce dimensionality and identify patterns in the data. The most important relevant reviews are extracted based on their ranking and cosine similarity.\n",
        "\n",
        "The report provides details on the algorithm and discusses the results and performance analysis. Two datasets, Opinosis and DUC, are used for evaluation. The evaluation is performed using the ROUGE metric and IR measures such as precision, recall, and F-measure. The proposed techniques show comparable results with human summaries and outperform previous baselines in terms of precision. Sample summaries are also provided to illustrate the outcomes.\n",
        "\n",
        "In conclusion, the proposed work offers approaches for opinion summarization, including abstractive-based and extractive-based techniques. The algorithms and evaluations demonstrate the effectiveness of the proposed methods in generating summaries from large corpus reviews.\n",
        "\n",
        "The code implementation is text summarization using **graph-based ranking**.\n",
        "The code follows a basic approach for generating summaries:\n",
        "\n",
        "1. It tokenizes the input text into sentences using the `sent_tokenize` function from NLTK.\n",
        "2. It builds a graph representation of the sentences, where each node represents a sentence and edges represent the similarity between sentences.\n",
        "3. The similarity between two sentences is calculated using cosine similarity based on the bag-of-words representation of the sentences.\n",
        "4. The graph is then ranked using the PageRank algorithm, assigning scores to each sentence.\n",
        "5. The sentences are sorted based on their scores, and the top-ranked sentences are selected to form the summary.\n",
        "6. Finally, the selected sentences are concatenated to generate the summary."
      ],
      "metadata": {
        "id": "p7H9W2ZiGVNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def build_graph(sentences):\n",
        "    graph = nx.Graph()\n",
        "    graph.add_nodes_from(range(len(sentences)))\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(i + 1, len(sentences)):\n",
        "            similarity_score = calculate_similarity(sentences[i], sentences[j])\n",
        "\n",
        "            if similarity_score > 0:\n",
        "                graph.add_edge(i, j, weight=similarity_score)\n",
        "\n",
        "    return graph\n",
        "\n",
        "def calculate_similarity(sentence1, sentence2):\n",
        "    vectorizer = CountVectorizer().fit_transform([sentence1, sentence2])\n",
        "    vectors = vectorizer.toarray()\n",
        "    similarity = cosine_similarity(vectors)\n",
        "\n",
        "    return similarity[0][1]\n",
        "\n",
        "def generate_summary(text, num_sentences):\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    if num_sentences >= len(sentences):\n",
        "        return text\n",
        "\n",
        "    graph = build_graph(sentences)\n",
        "    scores = nx.pagerank(graph)\n",
        "\n",
        "    ranked_sentences = sorted(((scores[i], i) for i in graph.nodes()), reverse=True)\n",
        "    summary_sentences = [sentences[idx] for _, idx in ranked_sentences[:num_sentences]]\n",
        "\n",
        "    summary = \" \".join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# Example usage\n",
        "text = \"Cerussite is a mineral consisting of lead carbonate (PbCO3), and is an important ore of lead. The name is from the Latin cerussa, white lead. Cerussa nativa was mentioned by Conrad Gessner in 1565, and in 1832 François Sulpice Beudant applied the name céruse to the mineral, while the present form, cerussite, is due to Wilhelm Karl Ritter von Haidinger in 1845. Miners' names for cerussite in early use were lead-spar and white-lead-ore. In a hydrate form known as white lead, the mineral is a key ingredient in lead paints and has also been used in cosmetics, but both uses are now discontinued in many places as a result of lead poisoning. These cerussite crystals, measuring approximately 4.0 cm × 3.0 cm × 2.0 cm (1.57 in × 1.18 in × 0.79 in), were found in a mine in Madan-e Nakhlak, Iran.\"\n",
        "summary = generate_summary(text, num_sentences=1)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zl_6bkcWTpc",
        "outputId": "100aa5a1-bae9-41bc-bfb1-fdff6f2ec585"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a hydrate form known as white lead, the mineral is a key ingredient in lead paints and has also been used in cosmetics, but both uses are now discontinued in many places as a result of lead poisoning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model using \"Opinosis\" dataset"
      ],
      "metadata": {
        "id": "3-br0F0icOl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMt1__HUewDQ",
        "outputId": "934a38f2-7a4f-4079-f6cb-09929f6cc3a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/gdrive/My Drive/Colab Notebooks/Reva/Project Text Summary\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48UCnNKnexp6",
        "outputId": "94e8c495-63fe-4ed4-f2e9-60e7fde24a4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Reva/Project Text Summary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWOwarEUfZhc",
        "outputId": "2d3052a1-041e-4b77-c7ac-d07490c3ffbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chardet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58e7WMab-Lhv",
        "outputId": "8fcef29a-94f6-4bb1-c249-d86e3dea3709"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from rouge import Rouge\n",
        "import chardet\n",
        "\n",
        "def load_opinosis_data(data_dir):\n",
        "    texts = []\n",
        "    summaries = []\n",
        "\n",
        "    # Iterate over the files in the topics directory\n",
        "    topics_dir = os.path.join(data_dir, 'topics')\n",
        "    for file_name in os.listdir(topics_dir):\n",
        "        file_path = os.path.join(topics_dir, file_name)\n",
        "        with open(file_path, 'rb') as file:\n",
        "            # Detect the file's encoding\n",
        "            raw_data = file.read()\n",
        "            encoding = chardet.detect(raw_data)['encoding']\n",
        "\n",
        "        with open(file_path, 'r', encoding=encoding) as file:\n",
        "            # Read the topic-based sentences\n",
        "            topic_sentences = file.readlines()\n",
        "            text = ' '.join(topic_sentences).strip()\n",
        "\n",
        "            # Get the corresponding topic name\n",
        "            temp = os.path.splitext(file_name)[0]\n",
        "            topic_name = os.path.splitext(temp)[0]\n",
        "\n",
        "            # Load the corresponding human-composed summaries\n",
        "            summaries_dir = os.path.join(data_dir, 'summaries-gold', topic_name)\n",
        "            for summary_file in os.listdir(summaries_dir):\n",
        "                summary_file_path = os.path.join(summaries_dir, summary_file)\n",
        "                with open(summary_file_path, 'r', encoding=encoding) as summary_file:\n",
        "                    # Read the human-composed summary\n",
        "                    summary = summary_file.readline().strip()\n",
        "\n",
        "                    texts.append(text)\n",
        "                    summaries.append(summary)\n",
        "\n",
        "    return texts, summaries\n",
        "\n",
        "\n",
        "def calculate_rouge_scores(reference_summaries, generated_summaries):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n",
        "    return scores\n",
        "\n",
        "def calculate_metrics(rouge_scores):\n",
        "    precision = rouge_scores['rouge-1']['p']\n",
        "    recall = rouge_scores['rouge-1']['r']\n",
        "    f1_score = rouge_scores['rouge-1']['f']\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "# Set the path to the Opinosis dataset directory\n",
        "opinosis_data_dir = '/content/gdrive/My Drive/Colab Notebooks/Reva/Project Text Summary/OpinosisDataset1.0_0'\n",
        "\n",
        "# Load Opinosis benchmark data\n",
        "reference_summaries, generated_summaries = load_opinosis_data(opinosis_data_dir)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_scores = calculate_rouge_scores(reference_summaries, generated_summaries)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision, recall, f1_score = calculate_metrics(rouge_scores)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFcXxot093y6",
        "outputId": "7ff5f515-093f-4a5a-dd3e-ba6d39dba228"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8751275677526471\n",
            "Recall: 0.01088230351487475\n",
            "F1-Score: 0.021428472283800692\n"
          ]
        }
      ]
    }
  ]
}